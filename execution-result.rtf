{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 rm(list=ls())\
> graphics.off()\
> \
> setwd("/Users/kd/Desktop/R/src")\
> \
> list.of.packages <- c("xgboost", "ParBayesianOptimization")\
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]\
> if(length(new.packages)) install.packages(new.packages)\
> \
> library(corrplot)\
> library(caret)\
> library(xgboost)\
> library("ParBayesianOptimization")\
> \
> \
> load("../data/spam_data_train.rda")\
> head(data_train)\
     word_freq_make word_freq_address word_freq_all word_freq_3d word_freq_our word_freq_over\
455             0.0               0.0          0.89            0          1.79           0.44\
2246            0.0               0.0          0.00            0          0.00           0.00\
1935            0.0               0.0          0.00            0          0.00           0.00\
1384            0.0               0.0          0.00            0          1.82           0.36\
679             0.1               0.1          0.70            0          0.60           0.20\
4129            0.0               0.0          0.00            0          0.00           0.00\
     word_freq_remove word_freq_internet word_freq_order word_freq_mail word_freq_receive\
455              0.00               0.00            0.00           0.00               0.0\
2246             0.00               0.00            0.00           0.00               0.0\
1935             0.00               0.00            0.00           0.00               0.0\
1384             0.36               0.72            0.36           0.36               0.0\
679              0.40               0.10            1.41           0.80               0.1\
4129             0.00               0.00            0.00           0.00               0.0\
     word_freq_will word_freq_people word_freq_report word_freq_addresses word_freq_free\
455            0.44                0                0                   0           1.34\
2246           0.00                0                0                   0           0.00\
1935           0.00                0                0                   0           0.00\
1384           0.00                0                0                   0           0.00\
679            0.50                0                0                   0           0.10\
4129           0.00                0                0                   0           0.00\
     word_freq_business word_freq_email word_freq_you word_freq_credit word_freq_your\
455                   0            0.00          2.24              0.0           4.48\
2246                  0            0.00          1.88              0.0           0.00\
1935                  0            0.00          0.00              0.0           0.00\
1384                  0            0.36          2.91              0.0           2.18\
679                   0            1.11          2.22              0.4           1.92\
4129                  0            0.00          5.81              0.0           1.16\
     word_freq_font word_freq_000 word_freq_money word_freq_hp word_freq_hpl word_freq_george\
455               0          0.00            0.00            0             0                0\
2246              0          0.00            0.00            0             0                0\
1935              0          0.00            0.00            0             0                0\
1384              0          0.72            0.00            0             0                0\
679               0          0.00            0.30            0             0                0\
4129              0          0.00            1.16            0             0                0\
     word_freq_650 word_freq_lab word_freq_labs word_freq_telnet word_freq_857 word_freq_data\
455              0             0              0                0             0              0\
2246             0             0              0                0             0              0\
1935             0             0              0                0             0              0\
1384             0             0              0                0             0              0\
679              0             0              0                0             0              0\
4129             0             0              0                0             0              0\
     word_freq_415 word_freq_85 word_freq_technology word_freq_1999 word_freq_parts word_freq_pm\
455              0         0.00                    0           0.00               0         0.00\
2246             0         1.88                    0           0.00               0         0.00\
1935             0         0.00                    0           1.96               0         1.96\
1384             0         0.00                    0           0.00               0         0.00\
679              0         0.10                    0           0.00               0         0.10\
4129             0         0.00                    0           0.00               0         0.00\
     word_freq_direct word_freq_cs word_freq_meeting word_freq_original word_freq_project\
455                 0            0              0.00               0.00              0.00\
2246                0            0              1.88               0.00              0.00\
1935                0            0              0.00               0.98              0.00\
1384                0            0              0.00               0.36              0.36\
679                 0            0              0.00               0.00              0.00\
4129                0            0              0.00               0.00              0.00\
     word_freq_re word_freq_edu word_freq_table word_freq_conference char_freq_; char_freq_(\
455             0          0.00               0                 0.00           0       0.073\
2246            0          0.00               0                 0.00           0       0.000\
1935            0          0.00               0                 0.98           0       0.377\
1384            0          0.00               0                 0.00           0       0.297\
679             0          0.00               0                 0.00           0       0.000\
4129            0          2.32               0                 0.00           0       0.163\
     char_freq_[ char_freq_! char_freq_$ char_freq_ capital_run_length_average\
455        0.000       0.000        0.00      0.000                      2.250\
2246       0.000       0.277        0.00      0.000                      3.000\
1935       0.000       0.125        0.00      0.000                      2.925\
1384       0.059       0.178        0.00      0.000                      2.446\
679        0.260       0.991        0.39      0.032                      3.173\
4129       0.000       0.490        0.00      0.000                      2.125\
     capital_run_length_longest capital_run_length_total label\
455                          12                      144     1\
2246                         17                       51     0\
1935                         27                      158     0\
1384                         11                      115     1\
679                          56                     1044     1\
4129                          7                       34     0\
> \
> # Correlation analysis\
> M <-cor(data_train)\
> corrplot(M, type="upper", order="hclust", tl.cex = 0.5)\
> findCorrelation(M, cutoff = .75, verbose = TRUE, names = TRUE)\
Compare row 32  and column  34 with corr  0.994 \
  Means:  0.142 vs 0.064 so flagging column 32 \
Compare row 34  and column  40 with corr  0.824 \
  Means:  0.126 vs 0.062 so flagging column 34 \
All correlations <= 0.75 \
[1] "word_freq_857" "word_freq_415"\
> \
> \
> # train/test split\
> smp_size <- floor(0.7 * nrow(data_train))\
> set.seed(123)\
> train_ind <- sample(seq_len(nrow(data_train)), size = smp_size)\
> \
> train <- data_train[train_ind, ]\
> test <- data_train[-train_ind, ]\
> \
> drop <- c('label')\
> \
> xtrain = as.matrix(train[,!(names(train) %in% drop)])\
> ytrain = train[,'label']\
> \
> xtest = as.matrix(test[,!(names(test) %in% drop)])\
> ytest = test[,'label']\
> \
> xgbTrain = xgb.DMatrix(data=xtrain,label=ytrain)\
> xgbTest = xgb.DMatrix(data=xtest,label=ytest)\
> \
> params = list(\
+   eta=0.001,\
+   max_depth=5,\
+   objective="binary:logistic",\
+   eval_metric="auc"\
+ )\
> model = xgb.train(\
+   params=params,\
+   data=xgbTrain,\
+   nrounds=1000,\
+   nthreads=1,\
+   early_stopping_rounds=50,\
+   watchlist=list(val1=xgbTrain,val2=xgbTest),\
+   verbose=1\
+ )\
[12:03:16] WARNING: amalgamation/../src/learner.cc:516: \
Parameters: \{ nthreads \} might not be used.\
\
  This may not be accurate due to some parameters are only used in language bindings but\
  passed down to XGBoost core.  Or some parameters are not used but slip through this\
  verification. Please open an issue if you find above cases.\
\
\
[1]	val1-auc:0.939109	val2-auc:0.927894 \
Multiple eval metrics are present. Will use val2_auc for early stopping.\
Will train until val2_auc hasn't improved in 50 rounds.\
\
[2]	val1-auc:0.938236	val2-auc:0.928507 \
[3]	val1-auc:0.938066	val2-auc:0.928893 \
[4]	val1-auc:0.938007	val2-auc:0.928816 \
[5]	val1-auc:0.938007	val2-auc:0.928767 \
[6]	val1-auc:0.938003	val2-auc:0.928809 \
[7]	val1-auc:0.938001	val2-auc:0.928809 \
[8]	val1-auc:0.937894	val2-auc:0.928907 \
[9]	val1-auc:0.937854	val2-auc:0.928935 \
[10]	val1-auc:0.937890	val2-auc:0.928886 \
[11]	val1-auc:0.937854	val2-auc:0.928935 \
[12]	val1-auc:0.937890	val2-auc:0.928886 \
[13]	val1-auc:0.937854	val2-auc:0.928935 \
[14]	val1-auc:0.937852	val2-auc:0.928935 \
[15]	val1-auc:0.937852	val2-auc:0.928935 \
[16]	val1-auc:0.937852	val2-auc:0.928935 \
[17]	val1-auc:0.937852	val2-auc:0.928935 \
[18]	val1-auc:0.937852	val2-auc:0.928935 \
[19]	val1-auc:0.937850	val2-auc:0.928935 \
[20]	val1-auc:0.937850	val2-auc:0.928935 \
[21]	val1-auc:0.937850	val2-auc:0.928935 \
[22]	val1-auc:0.937846	val2-auc:0.928935 \
[23]	val1-auc:0.937846	val2-auc:0.928935 \
[24]	val1-auc:0.937846	val2-auc:0.928935 \
[25]	val1-auc:0.937846	val2-auc:0.928935 \
[26]	val1-auc:0.937846	val2-auc:0.928935 \
[27]	val1-auc:0.937846	val2-auc:0.928935 \
[28]	val1-auc:0.937846	val2-auc:0.928935 \
[29]	val1-auc:0.937846	val2-auc:0.928935 \
[30]	val1-auc:0.937846	val2-auc:0.928935 \
[31]	val1-auc:0.937846	val2-auc:0.928935 \
[32]	val1-auc:0.937846	val2-auc:0.928935 \
[33]	val1-auc:0.937909	val2-auc:0.929008 \
[34]	val1-auc:0.943696	val2-auc:0.933814 \
[35]	val1-auc:0.943694	val2-auc:0.933797 \
[36]	val1-auc:0.943692	val2-auc:0.933790 \
[37]	val1-auc:0.943692	val2-auc:0.933790 \
[38]	val1-auc:0.943746	val2-auc:0.933804 \
[39]	val1-auc:0.943835	val2-auc:0.933797 \
[40]	val1-auc:0.943845	val2-auc:0.933783 \
[41]	val1-auc:0.943857	val2-auc:0.933776 \
[42]	val1-auc:0.943912	val2-auc:0.933930 \
[43]	val1-auc:0.943911	val2-auc:0.933930 \
[44]	val1-auc:0.943911	val2-auc:0.933930 \
[45]	val1-auc:0.943911	val2-auc:0.933930 \
[46]	val1-auc:0.944016	val2-auc:0.933951 \
[47]	val1-auc:0.944016	val2-auc:0.933951 \
[48]	val1-auc:0.944031	val2-auc:0.933965 \
[49]	val1-auc:0.943998	val2-auc:0.933972 \
[50]	val1-auc:0.944229	val2-auc:0.934098 \
[51]	val1-auc:0.944237	val2-auc:0.934133 \
[52]	val1-auc:0.944237	val2-auc:0.934133 \
[53]	val1-auc:0.944238	val2-auc:0.934140 \
[54]	val1-auc:0.944243	val2-auc:0.934133 \
[55]	val1-auc:0.944242	val2-auc:0.934140 \
[56]	val1-auc:0.944242	val2-auc:0.934140 \
[57]	val1-auc:0.944242	val2-auc:0.934140 \
[58]	val1-auc:0.944249	val2-auc:0.934140 \
[59]	val1-auc:0.944246	val2-auc:0.934140 \
[60]	val1-auc:0.944246	val2-auc:0.934140 \
[61]	val1-auc:0.944251	val2-auc:0.934126 \
[62]	val1-auc:0.944249	val2-auc:0.934112 \
[63]	val1-auc:0.944249	val2-auc:0.934105 \
[64]	val1-auc:0.944277	val2-auc:0.934133 \
[65]	val1-auc:0.944304	val2-auc:0.934140 \
[66]	val1-auc:0.944300	val2-auc:0.934126 \
[67]	val1-auc:0.944300	val2-auc:0.934126 \
[68]	val1-auc:0.944309	val2-auc:0.934112 \
[69]	val1-auc:0.944309	val2-auc:0.934133 \
[70]	val1-auc:0.944416	val2-auc:0.934245 \
[71]	val1-auc:0.944425	val2-auc:0.934245 \
[72]	val1-auc:0.944309	val2-auc:0.934133 \
[73]	val1-auc:0.944416	val2-auc:0.934245 \
[74]	val1-auc:0.944425	val2-auc:0.934245 \
[75]	val1-auc:0.944435	val2-auc:0.934245 \
[76]	val1-auc:0.944435	val2-auc:0.934245 \
[77]	val1-auc:0.944425	val2-auc:0.934245 \
[78]	val1-auc:0.944425	val2-auc:0.934245 \
[79]	val1-auc:0.944432	val2-auc:0.934245 \
[80]	val1-auc:0.944422	val2-auc:0.934245 \
[81]	val1-auc:0.944422	val2-auc:0.934245 \
[82]	val1-auc:0.944432	val2-auc:0.934245 \
[83]	val1-auc:0.944432	val2-auc:0.934245 \
[84]	val1-auc:0.944448	val2-auc:0.934224 \
[85]	val1-auc:0.944446	val2-auc:0.934266 \
[86]	val1-auc:0.944462	val2-auc:0.934203 \
[87]	val1-auc:0.944446	val2-auc:0.934182 \
[88]	val1-auc:0.944446	val2-auc:0.934182 \
[89]	val1-auc:0.944475	val2-auc:0.934238 \
[90]	val1-auc:0.944475	val2-auc:0.934238 \
[91]	val1-auc:0.944475	val2-auc:0.934238 \
[92]	val1-auc:0.944461	val2-auc:0.934203 \
[93]	val1-auc:0.944862	val2-auc:0.934648 \
[94]	val1-auc:0.944862	val2-auc:0.934648 \
[95]	val1-auc:0.944858	val2-auc:0.934648 \
[96]	val1-auc:0.944858	val2-auc:0.934648 \
[97]	val1-auc:0.944863	val2-auc:0.934648 \
[98]	val1-auc:0.944862	val2-auc:0.934648 \
[99]	val1-auc:0.944862	val2-auc:0.934648 \
[100]	val1-auc:0.945088	val2-auc:0.934816 \
[101]	val1-auc:0.945088	val2-auc:0.934816 \
[102]	val1-auc:0.945084	val2-auc:0.934816 \
[103]	val1-auc:0.945083	val2-auc:0.934816 \
[104]	val1-auc:0.945084	val2-auc:0.934788 \
[105]	val1-auc:0.945084	val2-auc:0.934788 \
[106]	val1-auc:0.945082	val2-auc:0.934788 \
[107]	val1-auc:0.945082	val2-auc:0.934676 \
[108]	val1-auc:0.945087	val2-auc:0.934655 \
[109]	val1-auc:0.945091	val2-auc:0.934683 \
[110]	val1-auc:0.945085	val2-auc:0.934683 \
[111]	val1-auc:0.945085	val2-auc:0.934683 \
[112]	val1-auc:0.945090	val2-auc:0.934693 \
[113]	val1-auc:0.945090	val2-auc:0.934693 \
[114]	val1-auc:0.945087	val2-auc:0.934693 \
[115]	val1-auc:0.945090	val2-auc:0.934988 \
[116]	val1-auc:0.945090	val2-auc:0.934967 \
[117]	val1-auc:0.945442	val2-auc:0.934970 \
[118]	val1-auc:0.945444	val2-auc:0.934949 \
[119]	val1-auc:0.945442	val2-auc:0.934949 \
[120]	val1-auc:0.945442	val2-auc:0.934970 \
[121]	val1-auc:0.945443	val2-auc:0.934970 \
[122]	val1-auc:0.945446	val2-auc:0.934970 \
[123]	val1-auc:0.945439	val2-auc:0.934977 \
[124]	val1-auc:0.945444	val2-auc:0.935082 \
[125]	val1-auc:0.945457	val2-auc:0.935117 \
[126]	val1-auc:0.945498	val2-auc:0.935180 \
[127]	val1-auc:0.945482	val2-auc:0.935173 \
[128]	val1-auc:0.945495	val2-auc:0.935096 \
[129]	val1-auc:0.945497	val2-auc:0.935082 \
[130]	val1-auc:0.945507	val2-auc:0.935117 \
[131]	val1-auc:0.945495	val2-auc:0.935082 \
[132]	val1-auc:0.945501	val2-auc:0.935082 \
[133]	val1-auc:0.945496	val2-auc:0.935082 \
[134]	val1-auc:0.945478	val2-auc:0.935103 \
[135]	val1-auc:0.945520	val2-auc:0.935110 \
[136]	val1-auc:0.945484	val2-auc:0.935103 \
[137]	val1-auc:0.945520	val2-auc:0.935110 \
[138]	val1-auc:0.945479	val2-auc:0.935103 \
[139]	val1-auc:0.945483	val2-auc:0.935103 \
[140]	val1-auc:0.945484	val2-auc:0.935103 \
[141]	val1-auc:0.945487	val2-auc:0.935110 \
[142]	val1-auc:0.945487	val2-auc:0.935103 \
[143]	val1-auc:0.945486	val2-auc:0.935103 \
[144]	val1-auc:0.945483	val2-auc:0.935103 \
[145]	val1-auc:0.945483	val2-auc:0.935110 \
[146]	val1-auc:0.945484	val2-auc:0.935110 \
[147]	val1-auc:0.945482	val2-auc:0.935103 \
[148]	val1-auc:0.945484	val2-auc:0.935103 \
[149]	val1-auc:0.945529	val2-auc:0.935215 \
[150]	val1-auc:0.945484	val2-auc:0.935110 \
[151]	val1-auc:0.945532	val2-auc:0.935215 \
[152]	val1-auc:0.945529	val2-auc:0.935215 \
[153]	val1-auc:0.945501	val2-auc:0.935215 \
[154]	val1-auc:0.945579	val2-auc:0.935215 \
[155]	val1-auc:0.945577	val2-auc:0.935222 \
[156]	val1-auc:0.945501	val2-auc:0.935222 \
[157]	val1-auc:0.945578	val2-auc:0.935222 \
[158]	val1-auc:0.945583	val2-auc:0.935222 \
[159]	val1-auc:0.945638	val2-auc:0.935243 \
[160]	val1-auc:0.945556	val2-auc:0.935243 \
[161]	val1-auc:0.945560	val2-auc:0.935243 \
[162]	val1-auc:0.945561	val2-auc:0.935243 \
[163]	val1-auc:0.945563	val2-auc:0.935243 \
[164]	val1-auc:0.945564	val2-auc:0.935355 \
[165]	val1-auc:0.945564	val2-auc:0.935236 \
[166]	val1-auc:0.945565	val2-auc:0.935355 \
[167]	val1-auc:0.945564	val2-auc:0.935355 \
[168]	val1-auc:0.945565	val2-auc:0.935355 \
[169]	val1-auc:0.945578	val2-auc:0.935348 \
[170]	val1-auc:0.945581	val2-auc:0.935355 \
[171]	val1-auc:0.945578	val2-auc:0.935355 \
[172]	val1-auc:0.945579	val2-auc:0.935355 \
[173]	val1-auc:0.945578	val2-auc:0.935348 \
[174]	val1-auc:0.945572	val2-auc:0.935355 \
[175]	val1-auc:0.945570	val2-auc:0.935348 \
[176]	val1-auc:0.945573	val2-auc:0.935355 \
[177]	val1-auc:0.945572	val2-auc:0.935348 \
[178]	val1-auc:0.945573	val2-auc:0.935355 \
[179]	val1-auc:0.945572	val2-auc:0.935348 \
[180]	val1-auc:0.945573	val2-auc:0.935355 \
[181]	val1-auc:0.945572	val2-auc:0.935348 \
[182]	val1-auc:0.945652	val2-auc:0.935369 \
[183]	val1-auc:0.945651	val2-auc:0.935362 \
[184]	val1-auc:0.945652	val2-auc:0.935369 \
[185]	val1-auc:0.945651	val2-auc:0.935369 \
[186]	val1-auc:0.945675	val2-auc:0.935369 \
[187]	val1-auc:0.945674	val2-auc:0.935362 \
[188]	val1-auc:0.945675	val2-auc:0.935376 \
[189]	val1-auc:0.945674	val2-auc:0.935376 \
[190]	val1-auc:0.945669	val2-auc:0.935404 \
[191]	val1-auc:0.945667	val2-auc:0.935397 \
[192]	val1-auc:0.945665	val2-auc:0.935404 \
[193]	val1-auc:0.945667	val2-auc:0.935404 \
[194]	val1-auc:0.945669	val2-auc:0.935404 \
[195]	val1-auc:0.945649	val2-auc:0.935376 \
[196]	val1-auc:0.945744	val2-auc:0.935489 \
[197]	val1-auc:0.945747	val2-auc:0.935481 \
[198]	val1-auc:0.945744	val2-auc:0.935489 \
[199]	val1-auc:0.945809	val2-auc:0.935481 \
[200]	val1-auc:0.945734	val2-auc:0.935489 \
[201]	val1-auc:0.945728	val2-auc:0.935481 \
[202]	val1-auc:0.945812	val2-auc:0.935489 \
[203]	val1-auc:0.945732	val2-auc:0.935489 \
[204]	val1-auc:0.945806	val2-auc:0.935489 \
[205]	val1-auc:0.945805	val2-auc:0.935481 \
[206]	val1-auc:0.945806	val2-auc:0.935503 \
[207]	val1-auc:0.945785	val2-auc:0.935622 \
[208]	val1-auc:0.945864	val2-auc:0.935622 \
[209]	val1-auc:0.945863	val2-auc:0.935615 \
[210]	val1-auc:0.945864	val2-auc:0.935622 \
[211]	val1-auc:0.945863	val2-auc:0.935622 \
[212]	val1-auc:0.945870	val2-auc:0.935643 \
[213]	val1-auc:0.945861	val2-auc:0.935615 \
[214]	val1-auc:0.945861	val2-auc:0.935622 \
[215]	val1-auc:0.945875	val2-auc:0.935643 \
[216]	val1-auc:0.945877	val2-auc:0.935643 \
[217]	val1-auc:0.945875	val2-auc:0.935636 \
[218]	val1-auc:0.945877	val2-auc:0.935643 \
[219]	val1-auc:0.945875	val2-auc:0.935636 \
[220]	val1-auc:0.945877	val2-auc:0.935643 \
[221]	val1-auc:0.945875	val2-auc:0.935636 \
[222]	val1-auc:0.945878	val2-auc:0.935643 \
[223]	val1-auc:0.945877	val2-auc:0.935636 \
[224]	val1-auc:0.945877	val2-auc:0.935643 \
[225]	val1-auc:0.946316	val2-auc:0.934851 \
[226]	val1-auc:0.946321	val2-auc:0.934858 \
[227]	val1-auc:0.946320	val2-auc:0.934851 \
[228]	val1-auc:0.946317	val2-auc:0.934858 \
[229]	val1-auc:0.946320	val2-auc:0.934858 \
[230]	val1-auc:0.946321	val2-auc:0.934858 \
[231]	val1-auc:0.946330	val2-auc:0.934851 \
[232]	val1-auc:0.946331	val2-auc:0.934858 \
[233]	val1-auc:0.946322	val2-auc:0.934893 \
[234]	val1-auc:0.946322	val2-auc:0.934886 \
[235]	val1-auc:0.946321	val2-auc:0.934879 \
[236]	val1-auc:0.946324	val2-auc:0.934886 \
[237]	val1-auc:0.946321	val2-auc:0.934886 \
[238]	val1-auc:0.946330	val2-auc:0.934893 \
[239]	val1-auc:0.946321	val2-auc:0.934879 \
[240]	val1-auc:0.946330	val2-auc:0.934886 \
[241]	val1-auc:0.946329	val2-auc:0.934879 \
[242]	val1-auc:0.946330	val2-auc:0.934886 \
[243]	val1-auc:0.946329	val2-auc:0.934879 \
[244]	val1-auc:0.946176	val2-auc:0.934886 \
[245]	val1-auc:0.946329	val2-auc:0.934879 \
[246]	val1-auc:0.946330	val2-auc:0.934886 \
[247]	val1-auc:0.946175	val2-auc:0.934886 \
[248]	val1-auc:0.946176	val2-auc:0.934886 \
[249]	val1-auc:0.946175	val2-auc:0.934879 \
[250]	val1-auc:0.946177	val2-auc:0.934893 \
[251]	val1-auc:0.946196	val2-auc:0.934907 \
[252]	val1-auc:0.946198	val2-auc:0.934907 \
[253]	val1-auc:0.946176	val2-auc:0.934886 \
[254]	val1-auc:0.946177	val2-auc:0.934893 \
[255]	val1-auc:0.946199	val2-auc:0.934907 \
[256]	val1-auc:0.946202	val2-auc:0.934907 \
[257]	val1-auc:0.946180	val2-auc:0.934886 \
[258]	val1-auc:0.946205	val2-auc:0.934914 \
[259]	val1-auc:0.946202	val2-auc:0.934900 \
[260]	val1-auc:0.946233	val2-auc:0.934907 \
[261]	val1-auc:0.946182	val2-auc:0.934893 \
[262]	val1-auc:0.946207	val2-auc:0.934921 \
Stopping. Best iteration:\
[212]	val1-auc:0.945870	val2-auc:0.935643\
\
> \
> # Review the final model and results\
> model\
##### xgb.Booster\
raw: 384.1 Kb \
call:\
  xgb.train(params = params, data = xgbTrain, nrounds = 1000, watchlist = list(val1 = xgbTrain, \
    val2 = xgbTest), verbose = 1, early_stopping_rounds = 50, \
    nthreads = 1)\
params (as set within xgb.train):\
  eta = "0.001", max_depth = "5", objective = "binary:logistic", eval_metric = "auc", nthreads = "1", validate_parameters = "TRUE"\
xgb.attributes:\
  best_iteration, best_msg, best_ntreelimit, best_score, niter\
callbacks:\
  cb.print.evaluation(period = print_every_n)\
  cb.evaluation.log()\
  cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, \
    verbose = verbose)\
# of features: 57 \
niter: 262\
best_iteration : 212 \
best_ntreelimit : 212 \
best_score : 0.935643 \
best_msg : [212]	val1-auc:0.945870	val2-auc:0.935643 \
nfeatures : 57 \
evaluation_log:\
    iter val1_auc val2_auc\
       1 0.939109 0.927894\
       2 0.938236 0.928507\
---                       \
     261 0.946182 0.934893\
     262 0.946207 0.934921\
> \
> # Create our prediction probabilities\
> pred <- predict(model, xgbTest)\
> pred_modf <- ifelse(pred >= 0.52, 1, 0)\
> \
> confusionMatrix(as.factor(pred_modf), as.factor(ytest), positive='1')\
Confusion Matrix and Statistics\
\
          Reference\
Prediction   0   1\
         0 456  57\
         1  23 241\
                                          \
               Accuracy : 0.897           \
                 95% CI : (0.8735, 0.9175)\
    No Information Rate : 0.6165          \
    P-Value [Acc > NIR] : < 2.2e-16       \
                                          \
                  Kappa : 0.7775          \
                                          \
 Mcnemar's Test P-Value : 0.0002247       \
                                          \
            Sensitivity : 0.8087          \
            Specificity : 0.9520          \
         Pos Pred Value : 0.9129          \
         Neg Pred Value : 0.8889          \
             Prevalence : 0.3835          \
         Detection Rate : 0.3102          \
   Detection Prevalence : 0.3398          \
      Balanced Accuracy : 0.8804          \
                                          \
       'Positive' Class : 1               \
                                          \
> \
> importance_matrix <- xgb.importance(colnames(xtrain), model = model)\
> \
> xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20, main="Feature importance")\
> \
> \
> \
> \
> # BO for hyper optimisation\
> \
> folds <- list(Fold1 = as.integer(seq(1,nrow(xtrain),by = 3)), \
+               Fold2 = as.integer(seq(2,nrow(xtrain),by = 3)), \
+               Fold3 = as.integer(seq(3,nrow(xtrain),by = 3)))\
> \
> scoringFunction <- function(eta, max_depth, min_child_weight, subsample) \{\
+   \
+   Pars <- list( \
+     booster = "gbtree"\
+     , eta = eta\
+     , max_depth = max_depth\
+     , min_child_weight = min_child_weight\
+     , subsample = subsample\
+     , objective = "binary:logistic"\
+     , eval_metric = "auc"\
+   )\
+   \
+   xgbcv <- xgb.cv(\
+     params = Pars\
+     , data = xgbTrain\
+     , nround = 5000\
+     , folds = folds\
+     , prediction = TRUE\
+     , showsd = TRUE\
+     , early_stopping_rounds = 100\
+     , maximize = TRUE\
+     , verbose = 0)\
+   \
+   return(\
+     list( \
+       Score = max(xgbcv$evaluation_log$test_auc_mean)\
+       , nrounds = xgbcv$best_iteration\
+     )\
+   )\
+ \}\
> \
> bounds <- list( \
+   eta = c(0.01, 0.2)\
+   , max_depth = c(5L, 10L)\
+   , min_child_weight = c(1, 25)\
+   , subsample = c(0.25, 1)\
+ )\
> set.seed(1234)\
> optObj <- bayesOpt(\
+   FUN = scoringFunction\
+   , bounds = bounds\
+   , initPoints = 6\
+   , iters.n = 3\
+ )\
\
Running initial scoring function 6 times in 1 thread(s)...  22.635 seconds\
\
Starting Epoch 1 \
  1) Fitting Gaussian Process...\
  2) Running local optimum search...        11.052 seconds\
  3) Running FUN 1 times in 1 thread(s)...  2.098 seconds\
\
Starting Epoch 2 \
  1) Fitting Gaussian Process...\
  2) Running local optimum search...        8.506 seconds\
  3) Running FUN 1 times in 1 thread(s)...  4.998 seconds\
\
Starting Epoch 3 \
  1) Fitting Gaussian Process...\
  2) Running local optimum search...        14.387 seconds\
  3) Running FUN 1 times in 1 thread(s)...  18.01 seconds\
> \
> optObj$scoreSummary\
   Epoch Iteration        eta max_depth min_child_weight subsample gpUtility acqOptimum inBounds\
1:     0         1 0.14423581         7        22.537067 0.7678269        NA      FALSE     TRUE\
2:     0         2 0.19070566         9         5.280210 0.4430337        NA      FALSE     TRUE\
3:     0         3 0.08308967         5         2.282578 0.2745218        NA      FALSE     TRUE\
4:     0         4 0.12110401         6        11.673982 0.6123226        NA      FALSE     TRUE\
5:     0         5 0.04330214         9        20.705602 0.9236875        NA      FALSE     TRUE\
6:     0         6 0.02787804         8        14.887639 0.6638588        NA      FALSE     TRUE\
7:     1         7 0.20000000         5        13.295551 0.2500000 0.4969894       TRUE     TRUE\
8:     2         8 0.12283823        10         1.000000 0.2500000 0.6496179       TRUE     TRUE\
9:     3         9 0.01000000         6         1.000000 0.5482204 0.5927259       TRUE     TRUE\
   Elapsed     Score nrounds errorMessage\
1:   1.911 0.9625237      35           NA\
2:   2.235 0.9723583      38           NA\
3:   4.215 0.9735573     193           NA\
4:   2.694 0.9685047      78           NA\
5:   3.842 0.9665883     135           NA\
6:   5.215 0.9661340     206           NA\
7:   1.624 0.9391503      66           NA\
8:   4.477 0.9730463      78           NA\
9:  17.546 0.9782577     687           NA\
> getBestPars(optObj)\
$eta\
[1] 0.01\
\
$max_depth\
[1] 6\
\
$min_child_weight\
[1] 1\
\
$subsample\
[1] 0.5482204}